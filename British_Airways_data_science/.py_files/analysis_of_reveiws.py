# -*- coding: utf-8 -*-
"""analysis_of-reveiws.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LG262mtjvT3UsC8fpse1U-yrwAD0rCIv

## Exploratory Data Analysis
"""

#imports

import pandas as pd
import numpy as np
import os

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import datetime as dt

from wordcloud import WordCloud, STOPWORDS

import warnings
warnings.filterwarnings("ignore")
warnings.warn("this will not show")

# import data
df = pd.read_csv("data/cleaned-BA-reviews.csv", index_col=0)

df

"""#### Average overall rating"""

# average rating
print(f"The average rating out of 10 is: {df.stars.mean()}")

"""#### Total counts for each ratings

"""

# count each rating
each_rating_total = df.stars.value_counts()

# plot the each rating total
each_rating_total.plot(kind="bar")
plt.xlabel("Ratings")
plt.ylabel("Total Number of reviews with that rating")
plt.title("Counts for each ratings")
plt.savefig("image/each_rating_total.png", dpi=300)
plt.show()

"""#### Number of feedbacks with each rating"""

df_ratings = pd.DataFrame(each_rating_total)
df_ratings

df_ratings.keys()

df_ratings.reset_index(inplace=True)

df_ratings

df_ratings.keys()

# renaming the columns
df_ratings.rename(columns={'stars':'Stars', 'count':'total_counts'}, inplace=True)

df_ratings

# calculating pct values, and set as a column
pct_values = (df_ratings.total_counts.values/ df_ratings.total_counts.values.sum() *100).tolist()
pct_values = [round(x,2) for x in pct_values]
df_ratings['pct_values'] = pct_values

#resetting index as we do not want to confuse between the index and the rating values
#df_ratings = df_ratings.reset_index()

# renaming columns
#df_ratings.rename(columns={'stars':'Stars', 'count':'total_counts'}, inplace=True)

df_ratings

# plot each rating with total number of reviews
clrs = ['Red' if (x ==  max(df_ratings.total_counts)) else 'g' for x in df_ratings.total_counts ]

ax = sns.barplot(x=df_ratings.Stars, y=df_ratings.total_counts, data=df_ratings, errwidth=0,palette=clrs)
ax.bar_label(ax.containers[0])
ax.set_xlabel("Ratings")
ax.set_ylabel("Number of reviews with that rating")
ax.set_title("Reviews for each ratings")
plt.savefig("image/each_rating_total_with_total_reviews.png", dpi=300)
plt.show()

# number of unique countries to give the reviews
print(f"Unique countries: {len(df.country.unique())}")

"""#### Country with the most review  """

# reviewer country
reviewer_country = df.country.value_counts()

reviewer_country

# setting first five country as a reviewer dataframe
df_country_review = pd.DataFrame(reviewer_country.head())

df_country_review.keys()

df_country_review

# resetting index
df_country_review.reset_index(inplace=True)

df_country_review.keys()

df_country_review

# renaming the column
df_country_review.rename(columns={'count':'total_reviews'}, inplace=True)

df_country_review

# review by country
df_country_review.plot(kind="bar", x='country')
plt.title("Review by country")
plt.xticks(rotation=45)
plt.savefig("image/review_by_country.png", dpi=300)
plt.grid()
plt.show()

"""#### Country with average highest ratings"""

average_rating = df.groupby('country')['stars'].mean()
average_rating

# sort values
average_rating = average_rating.sort_values(ascending=False)

average_rating

df_country_rating = pd.DataFrame(average_rating)

df_country_rating

df_country_rating.keys()

df_country_rating.reset_index(inplace=True)

df_country_rating.keys()

df_country_rating

# renaming column name
df_country_rating.rename(columns={'stars':'avg_rating'}, inplace=True)

# plot the image: average rating by country
fig, ax = plt.subplots(figsize=(14,4))
ax1 = sns.barplot(x='country', y='avg_rating', data=df_country_rating[:12])
ax.bar_label(ax.containers[0])
ax.set_title("Top 12 Countries with avg highest rating")
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
#plt.grid()
plt.savefig("image/avg_rating_by_country.png", dpi=300)
plt.show()

"""#### Time Series Analysis"""

# date to datetime
df.date = pd.to_datetime(df.date)

fig = px.line(df, x='date', y="stars")
fig.update_xaxes(rangeslider_visible=True)
fig.show()

"""**Findings:** It can be seen that between April 2020 to August 2021 there has been a decline in reviews due to **Covid Pandemic travel restrictions**. **No particular trend** is visible from the plot."""

import nltk
from nltk.corpus import stopwords

# Download the stopwords resource
nltk.download('stopwords')

# Start with one review:
reviews = " ".join(df.corpus)
plt.figure(figsize=(20,10))

# Now you can safely use stopwords.words('english')
stopwords = set(stopwords.words('english'))

# Create and generate a word cloud image:
wordcloud = WordCloud(height=600,width=600,max_font_size=100, max_words=500, stopwords=stopwords).generate(reviews)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig("image/wordcloud.png", dpi=300)
plt.show()

"""**Not set the idea of whether the review is positive or negative**.

"""

import nltk
from nltk.corpus import stopwords

reviews = " ".join(df.corpus)
plt.figure(figsize=(20,10))

stopwords = set(stopwords.words('english'))
stopwords.update(["ba","flight", "british","airway", "airline","plane", "told","also","passenger" \
                 "london", "heathrow", "aircraft", "could","even", "would"])
# Create and generate a word cloud image:
wordcloud = WordCloud(height=500,width=500,max_font_size=100, max_words=300, stopwords=stopwords).generate(reviews)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""## Word Frequency"""

from nltk import ngrams
from nltk.probability import FreqDist

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer

# split the text of all reviews into a list of words
words = reviews.split(" ")

# remove certain words that will not be used to determine the positive or negative sentiment
stopwords = text.ENGLISH_STOP_WORDS.union(['flight', 'ba', "passenger","u", "london","airway","british","airline",\
                                           "heathrow","plane","lhr","review"])


new_words = [word for word in words if word not in stopwords]

nlp_words=FreqDist(new_words).most_common(20)

# create a dataframe of these word and its frequencies
all_fdist = pd.Series(dict(nlp_words))

# Setting figure, ax into variables
fig, ax = plt.subplots(figsize=(15,8))

# Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing
all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)
all_plot.bar_label(all_plot.containers[0])
plt.xticks(rotation=30)
plt.title("Word Frequency")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.savefig("image/word_frequency.png", dpi=300)
plt.show()

"""**Seat is most talked about the airline followed by Service and food:**we still do not know is how they are expressing about each of this service. Use **ngram plots** to see if they are **bad or good** in experience.

## Word Frequency with N-gram
"""

# import modules
import nltk.collocations as collocations
from nltk import FreqDist, bigrams

reviews = " ".join(df.corpus)

# split the text of all reviews into a list of words
words = reviews.split(" ")

new_words = [word for word in words if word not in stopwords]

def get_freq_dist(new_words, number_of_ngrams ):
    from nltk import ngrams

    # Generate bigrams
    ngrams = ngrams(new_words, number_of_ngrams)

    # Creating FreqDist
    ngram_fd = FreqDist(ngrams).most_common(40)

    # Sort values by highest frequency
    ngram_sorted = {k:v for k,v in sorted(ngram_fd, key=lambda item:item[1])}

    # Join bigram tokens with '_' + maintain sorting
    ngram_joined = {'_'.join(k):v for k,v in sorted(ngram_fd, key=lambda item:item[1])}

    # Convert to Pandas series for easy plotting
    ngram_freqdist = pd.Series(ngram_joined)
    plt.figure(figsize=(10,10))
    ax = ngram_freqdist.plot(kind="barh")
    ax.bar_label(ax.containers[0])
    plt.title(f"{number_of_ngrams}-gram Frequency")
    plt.show()

    return ax

get_freq_dist(new_words,4)

"""##### Findings:
  - Positive terms regarding cabin crew     
     - cabin_crew_friendly_helpful
     - cabin_crew_friendly_attentive
     - cabin_crew_friendly_efficient
  - Cutomers are providing good reviews about cabin crew staff

We will group the reviews based on ratings for better idea.
- Assume
  - ratigs 1-3 are bad reviews
  - 4-6 are average/good experience
  - 7-10 indicates a great experience
"""

ratings_1_3 = df[df.stars.isin([1,2,3])]
ratings_4_6 = df[df.stars.isin([4,5,6])]
ratings_7_10 = df[df.stars.isin([7,8,9,10])]

reviews_1_3 = " ".join(ratings_1_3.corpus)
reviews_4_6 = " ".join(ratings_4_6.corpus)
reviews_7_10 = " ".join(ratings_7_10.corpus)

#split the text of all reviews into a list of words
words_1_3 = reviews_1_3.split(" ")
words_4_6 = reviews_4_6.split(" ")
words_7_10 = reviews_7_10.split(" ")


new_words_7_10 = [word for word in words_7_10 if word not in stopwords]

get_freq_dist(new_words_7_10,4)

new_words = [word for word in words_4_6 if word not in stopwords]

get_freq_dist(new_words,4)

new_words = [word for word in words_1_3 if word not in stopwords]

get_freq_dist(new_words,4)

"""****

**Note:** textblob library to define if the text is negative or positive and to what extent.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from textblob import TextBlob
# from textblob.sentiments import NaiveBayesAnalyzer
# 
# #set a column Polarity with all 0 values initially
# df['polarity'] = 0
# 
# # Iterate using df.iterrows() to avoid index issues
# for index, row in df.iterrows():
#     sent = TextBlob(row['corpus'])
#     polarity = sent.sentiment.polarity
#     subjectivity = sent.sentiment.subjectivity
#     df.loc[index, 'polarity'] = polarity # Use .loc for safe assignment

# let's see how many texts are with positive comments

print(f"{df[(df['polarity'] >-0.2) & (df['polarity'] <0.2)].shape[0]} number of reviews between -0.2 and 0.2 polarity score")

print(f"{df[(df['polarity'] >-0.1) & (df['polarity'] <0.1)].shape[0]} number of reviews between -0.1 and 0.1 polarity score")

"""Polarity score is given between **-1 to 1** and more close the value to -1, it **indicates negative review** and vice versa is true for positive value. If we consider a threshold where any review with polarity greater than 0.2 is positive and less than -0.2 is negative, we are left with 2286 reviews that lies in the neutral zone. To further narrow down this number of neutral reviews, let's take the threshold of 0.1.

We will try another method of **labelling the reveiws as positives or negatives.** In this we will use **VADER algorihtm** by nltk library.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import nltk
# 
# # Download the VADER lexicon
# nltk.download('vader_lexicon')
# 
# from nltk.sentiment.vader import SentimentIntensityAnalyzer
# vds = SentimentIntensityAnalyzer()
# 
# 
# #set a column Polarity with all 0 values initially
# df['label'] = 0
# 
# # Iterate through the dataframe using .iterrows() to access rows and index
# for index, row in df.iterrows():
# 
#     # Access 'corpus' column from the row using .loc or ['corpus']
#     score = vds.polarity_scores(row['corpus'])['compound']
# 
#     if score > 0.2:
#         df.loc[index, 'label'] = 1
#     elif score < 0:
#         df.loc[index, 'label'] = -1
#     else:
#         df.loc[index, 'label'] = 0

df.label.value_counts()

"""## Topic Modeling with LDA
We have already cleaned our reviews, however, for topic modeling we will also require word embeddings or (words matrix). For this purpose we will use **count vectorizer method** from sklearn library.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from sklearn.feature_extraction.text import CountVectorizer
# 
# # create an object of count vectorizer
# vect = CountVectorizer()
# 
# # apply transformation
# tf = vect.fit_transform(df.corpus).toarray()
# tf_feature_names = vect.get_feature_names_out()

# import LDA module
from sklearn.decomposition import LatentDirichletAllocation

# declare the number of topics
number_of_topics = 8

model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)

# fit the term frequency data to the model
model.fit(tf)

# create empty dictionary to store key value pair of topic number and its weights
topic_dict = {}

# loop through model components
for topic_idx, topic in enumerate(model.components_):
    topic_dict["Topic %d words" % (topic_idx)]= ['{}'.format(tf_feature_names[i])
            for i in topic.argsort()[:-10 - 1:-1]]
    topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])
            for i in topic.argsort()[:-10 - 1:-1]]

df_topic =pd.DataFrame(topic_dict)

df_topic

"""There are certain words that are not useful to detect the tone of the reviews; for example **britsh airways, passenger, flight.**

## Topic modeling with NMF
"""

# import module
from sklearn.decomposition import NMF

nmf = NMF(n_components=2, init='random', random_state=0)
nmf.fit_transform(tf)

topic_dict = {}

#loop through model components
for topic_idx, topic in enumerate(nmf.components_):
    topic_dict["Topic %d words" % (topic_idx)]= ['{}'.format(tf_feature_names[i])
            for i in topic.argsort()[:-10 - 1:-1]]
    topic_dict["Topic %d weights" % (topic_idx)]= ['{:.1f}'.format(topic[i])
            for i in topic.argsort()[:-10 - 1:-1]]

df_topic =pd.DataFrame(topic_dict)

df_topic

"""With NMF algorithm as well we see there are few words that are not conclusive; **seat**."""